As related work, we consider recent advances in both machine learning infrastructure and serverless computing domains. In the former area, much research has extended efforts into designing efficient systems for inference and deployment of machine learning models. As a complement to the Tensorflow framework, Tensorflow-serving~\cite{ref:tensorflow-serving} integrates new models and updates versions from training to serving. Though it makes seminal exploration on the multi-tenant model hosting service, Tensorflow-serving does not realize authentic high-performing parallelism to handle concurrent heavy query loads. 

Clipper~\cite{ref:clipper} constructs a general-purpose low-latency prediction serving system, which attempts to solve the problem of demanding real-time prediction at the client-side and handling heavy query load at the server-side. It also enables the model composition and online learning to improve accuracy and render more reliable predictions. To explore the multi-pipeline techniques, PRETZEL~\cite{ref:pretzel} casts model-serving as a database problem and applies multi-query optimizations to maximize performance. However, both Clipper and PRETZEL require considerable compute resources in caching, batching, adaptive model selection and off-line training to maximize throughput. Therefore, they are not optimized for resource constrained and heterogeneous, multi-tier IoT systems. 
To the best of our knowledge, STOIC is the first work to addresses this problem by integrating machine learning applications into a serverless architecture that leverages GPU as additional computational resources for IoT devices. We consider it as a promising and extensible solution for high-throughput and low-latency system for online training and machine learning applications in general. 

 To build an end-to-end system for practical machine learning applications, we require several other components. Seneca~\cite{ref:seneca} fine-tunes hyper-parameters of machine learning models on a general-purpose serverless architecture (AWS Lambda~\cite{ref:lambda}). It provides a fast and low-cost method to grid search for the best-performing hyper-parameter set, which is essential in the deployment pipeline of machine learning applications. Velox~\cite{ref:velox} offers a low-latency and scalable solution for complex analytical model-serving, in which it completes a missing piece of personalized prediction serving using Apache Spark~\cite{ref:spark}. For calibrating performance, McGrath et al.~\cite{ref:serverless} propose an empirical methodology to measure the design and performance of serverless platforms, including latency and auto-scaling capability. These related systems are complementary to STOIC and can be combined together
to provide a robust serverless ecosystem for machine learning applications.
