As related work, we consider recent advances in both machine learning infrastructure and serverless computing domains. In the former area, much research have extended efforts into designing efficient systems for inference and deployment of machine learning models. As a complement to tensorflow framework, Tensorflow-serving~\cite{ref:tensorflow-serving} integrates new models and updates versions from training to serving. Though it makes seminal exploration on the multi-tenant model hosting service, Tensorflow-serving does not realize authentic high-performing parallelism to handle concurrent heavy query loads. On that note, Clipper~\cite{ref:clipper} constructs a general-purpose low-latency prediction serving system, which attempts to solve the problem of demanding real-time prediction at the client side and handling heavy query load at the server side. It also enables the model composition and online learning to improve the accuracy and render more reliable predictions. To explore the multi-pipeline techniques, PRETZEL~\cite{ref:pretzel} casts model serving as a database problem and applies multi-query optimizations to maximize performance. However, both Clipper and PRETZEL could demand considerable compute resources in caching, batching, adaptive model selection and off-line training to maximize throughput. Therefore, they are not optimized for resource-constraint and heterogeneous IoT devices. To the best of our knowledge, STOIC is the first work to addresses this problem by integrating machine learning application into serverless architecture that leverages GPU as additional computational resource for IoT devices. We consider it as a promising and extensible solution for high-throughput and low-latency system for online training and machine learning applications in general. 

 To build up an end-to-end system for practical machine learning application, we need several other components that bring it to fruition. Seneca~\cite{ref:seneca} fine-tunes hyper-parameters of machine learning models on a general-purpose serverless architecture, namely AWS Lambda~\cite{ref:lambda}. It provides a fast and low-cost method to grid search for the best-performing hyperparameter set, which is essential in the deployment pipeline of machine learning application. Velox~\cite{ref:velox} offers a low-latency and scalable solution for complex analytical model-serving, in which it completes a missing piece of personalizaed prediction serving using Apache Spark~\cite{ref:spark}. For calibrating the performance, McGrath et al.~\cite{ref:serverless} propose a empirical methodology to measure the design and performance of serverless platform, including latency and auto-scaling capability. All these precursory work completes the serverless ecosystem, particularly for better training, inference and serving machine learning applications.