In this section, we empirically evaluate STOIC's performance on executing machine learning applications, comparing with single-runtime systems. We first outline the machine learning application benchmark that we consider and our experimental methodology. The result is then presented.

\subsection{Benchmark Application and Dataset}

We benchmark STOIC using an image processing application that classifies animal images from a wildlife monitoring system called ``Where's The Bear" (WTB)~\cite{ref:wtb}. ``Where's The Bear" is an end-to-end distributed data acquisition and analytics system that implements an IoT architecture and edge cloud. Our application inferences streaming photos taken by wildly deployed camera traps in Sedgwick Natural Reserve using a convolutional neural network (CNN)~\cite{ref:cnn} model trained by labeled images from the WTB dataset. Technically, it employs Tensorflow~\cite{ref:tensorflow} and Scikit-learn~\cite{ref:scikit} machine learning frameworks to perform the image classification.  

 In total, there are five classes that we consider in the CNN model training: Bird, Fox, Rodent, Human and Empty. Since the volume of classes are unbalanced due to different occurring frequency of animals, we up-sample the minority class (e.g. fox) by Keras ImageDataGenerator~\cite{ref:keras} to ensure the classification model is not biased. Every image in the WTB dataset is resized to $1920 \times 1080$, and for each class, the dataset contains 251 images used to train CNN model. Once the model is trained, the application caches this model in hdf5 format and store it both at Edge Cloud disk storage and a shared volume in Ceph file system at Nautilus Cloud. 

\subsection{Application Efficacy}

\begin{table}[t] \centering 
\input{tables/validation}
\caption{\textbf{Mean and stdev of total response time~($T_r$) and processing time~($T_p$) of 40-image batch}: STOIC schedules tasks onto the runtime (\textit{gpu1}) that has the least total response time.
\label{tab:validation}}
\end{table}

We first test the efficacy of STOIC by processing an image batch of fixed size at four runtimes individually and then compare with STOIC. To make the result reliable, we again conduct the experiment 10 times and list the mean and standard deviation of total response time~($T_r$) and processing time~($T_p$) in Table~\ref{tab:validation}. We can clearly see from the table that STOIC schedules 40-image batch to \textit{gpu1} runtime, based on its prediction that  \textit{gpu1} would have the least total response time~($T_r$). One important observation is that \textit{gpu2} runtime has even lower processing time~($T_p$) than \textit{gpu1}, but STOIC disregard \textit{gpu2} in this scenario, because its gain in processing time~($T_p$) does not compensate its lengthy deployment time~($T_d$) on Nautilus Cloud.

\begin{figure}[t] \centering 
\includegraphics[scale=0.32]{response-time}
\caption{\textbf{Total Response Time~($T_r$) of image batches of growing sizes}: The x-axis represents batch size, while y-axis is the total response time~($T_r$). STOIC, which is depicted in blue dashed line, schedules the task on the runtime with the least total response time.  
\label{fig:response-time}}
\end{figure}

Additionally, we test STOIC by processing a series of image batches of growing sizes both on four runtimes and STOIC. The progressive response times are depicted in Figure~\ref{fig:response-time}. The x-axis is the size of image batch and the y-axis is the total response time~($T_r$) in seconds. The red curve represents the linearly increasing response time from \textit{edge} runtime, whereas the yellow one depicts the \textit{cpu} runtime at Nautilus Cloud. We observe that its slope is more moderate than \textit{edge} runtime since CPUs in nodes of Nautilus Cloud are usually more powerful than Edge Cloud. The pink and green curve represent the \textit{gpu1} and \textit{gpu2} runtime respectively and they intersect at the batch size of 95, at which STOIC would switch the deployment of task from \textit{gpu1} to \textit{gpu2}. The blue dashed line depicts the total response time~($T_r$) of STOIC, which is able to schedule a series of task to the runtime with the least latency. According to such result, we ensure STOIC improves system performance by dynamic scheduling based on image batch size.


\begin{figure}[t] \centering 
\includegraphics[scale=0.42]{figures/24-batches}
\caption{\textbf{Average total response time~($T_r$) of 24-hour empirical dataset}: The x-axis represents runtimes, while y-axis represents the average total response time~($T_r$) of 24-hour dataset. The 24-hour batch sizes are generated from the distribution of historical data. 
\label{fig:24-batch}}
\end{figure}


\subsection{Empirical Experiment}

As a empirical evaluation of STOIC, we compare the total response time~($T_r$) of a series of image batches of varying sizes by STOIC and four single runtimes. To accelerate the repetitive experiment, we developed a simulator to generate image batches based on the frequency distribution of WTB dataset. According to 2016 WTB dataset, the size of image batch fits to normal distribution $\mathbf{N}(\mu = 42.75, \sigma^2 = 39.5)$, thus, the simulator generates 24 image batches in the Edge Cloud to emulate streaming data in one day from open field camera traps. To conduct unbiased evaluation, we seed the simulator to make these 24 image batches consistent across all runtimes and STOIC. 

To ensure the validity of outcome, We again run such experiment 10 times for each runtime scenario and report the average value. Figure~\ref{fig:24-batch} demonstrates the average total response time~($T_r$) for four individual runtimes and STOIC. In comparison, STOIC obviously has the lowest average latency in our experiment. According to such result, we conclude that STOIC outperforms single-runtime scheduling mechanism on empirical dataset and real-world machine learning application.



